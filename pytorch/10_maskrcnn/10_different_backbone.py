# -*- coding: utf-8 -*-

"""
pytorch.ipynb

Automatically generated by Colaboratory.

"""

# refer to https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#defining-the-dataset

import torchvision

backbone = torchvision.models.mobilenet_v2(pretrained=True).features
# load a pre-trained model for classification and return
# only the features, do not need the classifier
# MobileNetV2(
#   (features): Sequential(
#     (0): ConvBNActivation(
#       (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
#       (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
#       (2): ReLU6(inplace=True)
#     )
#     ...
#     (18): ConvBNActivation(
#       (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
#       (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
#       (2): ReLU6(inplace=True)
#     )
#   )
#   (classifier): Sequential(
#     (0): Dropout(p=0.2, inplace=False)
#     (1): Linear(in_features=1280, out_features=1000, bias=True)
#   )
# )

# FasterRCNN needs to define the backbone.out_channels.
# and for MobileNetV2, the final out_channel = 1280
backbone.out_channels = 1280 

# define a RPN generator, with 5 sizes and 3 ratios
# can generate 5x3 anchors per spatial location
from torchvision.models.detection.rpn import AnchorGenerator
anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),
                                   aspect_ratios=((0.5, 1.0, 2.0),))

# let's define what are the feature maps that we will
# use to perform the region of interest cropping, as well as
# the size of the crop after rescaling.
# if your backbone returns a Tensor, featmap_names is expected to
# be [0]. More generally, the backbone should return an
# OrderedDict[Tensor], and in featmap_names you can choose which
# feature maps to use.
roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],
                                                output_size=7,
                                                sampling_ratio=2)

from torchvision.models.detection import FasterRCNN
model = FasterRCNN(backbone, 
                   num_classes=2, 
                   rpn_anchor_generator=anchor_generator,
                   box_roi_pool=roi_pooler)
