# -*- coding: utf-8 -*-
"""
pytorch.ipynb

Automatically generated by Colaboratory.

"""

import torch.nn as nn
import torch
import matplotlib.pyplot as plt


# Activation function
# =====================
# torch.nn.ReLU()
# torch tanh()
# torch sigmoid()
# torch.nn.Softmax()

x = torch.range(-5., 5., 0.1)
y1 = torch.sigmoid(x)
y2 = torch.tanh(x)

relu = torch.nn.ReLU()
y3 = relu(x)

plt.plot(x.numpy(), y1.numpy())
plt.plot(x.numpy(), y2.numpy())
plt.plot(x.numpy(), y3.numpy())

# torch.nn.Softmax()
# torch.randn()
# torch.sum()

softmax = nn.Softmax(dim=1)
x = torch.randn(1,3)
y = softmax(x)
print(x)
print(y)
print(torch.sum(y, dim=1))  # sum need to be 1.
print("")

# Loss function
# =====================
# torch.nn.MSELoss()             Normally for regression
# torch.nn.CrossEntropyLoss()    Normally for classification
# torch.tensor()                 Only Tensors of floating point and complex dtype can require gradients

loss_MSE = nn.MSELoss()
outputs = torch.tensor([1.,2.,2.], requires_grad=True)  
targets = torch.tensor([1.,2.,3.])   # mean = 2, MSE = (1/3) * (3-2)^2
print(outputs)
print(targets)
loss = loss_MSE(outputs, targets)
print(loss)
print("")


loss_CE = nn.CrossEntropyLoss()
outputs = torch.randn([2,3], requires_grad=True)
targets = torch.tensor([1,0], dtype=torch.int64)  # need to use int64
print(outputs)
print(targets)
loss = loss_CE(outputs, targets)
print(loss)

